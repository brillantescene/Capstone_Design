{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "soundee-major_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqUjAPlZbSKlcQZeHIaa+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brillantescene/Capstone_Design/blob/master/cnn/soundee_major_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMs6v3E1ax6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRHnQAUHP3By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5CAPQ2XP3EU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "94231bda-0440-4eaf-f1b4-22b70287277d"
      },
      "source": [
        "from tensorflow import keras #import keras\n",
        "from tensorflow.keras import Model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9nE5p4gP3Gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecS41D_4P3J5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pathlib"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Im2mo7w5Du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD_HphJgSdRI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "18d3a397-2a1b-41fd-a3c1-a74c92a84e6e"
      },
      "source": [
        "data_dir = tf.keras.utils.get_file(origin='https://github.com/brillantescene/Capstone_Design/raw/master/melspetrogram_img.tgz', \n",
        "                                   fname='images', extract=True)\n",
        "data_dir = pathlib.Path(data_dir)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/brillantescene/Capstone_Design/raw/master/melspetrogram_img.tgz\n",
            "204619776/204612094 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXHk7J3HcIhI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba5b735b-aa86-4068-a4a7-a21788d469d8"
      },
      "source": [
        "data_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.keras/datasets/images')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNGLpY40SdTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = pathlib.PosixPath(\"/root/.keras/datasets/melspetrogram_img\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpo3m_T9SdUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6700f8b-13e3-474c-ba11-014dc419239b"
      },
      "source": [
        "image_count = len(list(data_dir.glob('*/*.png')))\n",
        "image_count"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4217"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp90oMiIf9Jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "47351135-26de-47d6-ad69-b8e7090279bc"
      },
      "source": [
        "!ls  /root/.keras/datasets/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images\tmelspetrogram_img\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5J_bbsXSdWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*')])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-M_9yUf0sqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "168129fe-8910-4405-ee62-2fb4c38a4285"
      },
      "source": [
        "CLASS_NAMES"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['water', 'drop', 'motor'], dtype='<U5')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJvURNE9TNmh",
        "colab_type": "text"
      },
      "source": [
        "tf.data(), train, validation, test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8hwdNcqSdYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7t-VxfwSdbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for f in list_ds.take(5):\n",
        "#  print(f.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhRtByxhP3NF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_num = int(image_count * 0.7)\n",
        "test_num = int(image_count * 0.2)\n",
        "val_num = int(image_count * 0.1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NksBdvjIP3Qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "training_epochs = 100\n",
        "BATCH_SIZE = 64\n",
        "IMG_HEIGHT = 224 #770\n",
        "IMG_WIDTH = 224 #310\n",
        "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
        "VALIDATION_STEPS = np.ceil(val_num/BATCH_SIZE)\n",
        "TEST_STEPS = np.ceil(test_num/BATCH_SIZE)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOTHaZ8aUedu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  return parts[-2] == CLASS_NAMES\n",
        "\n",
        "def decode_img(img):\n",
        "  img = tf.image.decode_png(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
        "\n",
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # 파일에서 raw data를 문자열로 로드\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ltiwStYUegW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Cil1cdUeju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for image, label in labeled_ds.take(1):\n",
        "#  print(\"Image shape: \", image.numpy().shape)\n",
        "#  print(\"Label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EvG9_nkUemd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ = labeled_ds.take(train_num)\n",
        "test_ = labeled_ds.take(test_num)\n",
        "val_ = labeled_ds.take(val_num)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ0STgoWbSEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b997a9c-5926-4ddf-808b-4b8bda566539"
      },
      "source": [
        "train_\n",
        "test_\n",
        "val_"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((224, 224, 3), (3,)), types: (tf.float32, tf.bool)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpuWodFiUep1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "  ds = ds.repeat()\n",
        "  ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  return ds"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uYBpmEeVAEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = prepare_for_training(train_)\n",
        "test_ds = prepare_for_training(test_)\n",
        "val_ds = prepare_for_training(val_)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw1WO8C8bXhS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4543eb78-0f20-4edf-f503-601621bdeda3"
      },
      "source": [
        "train_ds\n",
        "test_ds\n",
        "val_ds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, 310, 770, 3), (None, 5)), types: (tf.float32, tf.bool)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LJiO5b0XCwp",
        "colab_type": "text"
      },
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmYpQzgmwnVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AcousticSoundModel():\n",
        "  inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "\n",
        "  conv1 = Conv2D(filters=64, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)(inputs)\n",
        "  drop1 = Dropout(rate=0.2)(conv1)\n",
        "  pool1 = MaxPool2D(padding='SAME')(drop1)\n",
        "\n",
        "  conv2 = Conv2D(filters=64, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)(pool1)\n",
        "  drop2 = Dropout(rate=0.2)(conv2)\n",
        "  pool2 = MaxPool2D(padding='SAME')(drop2)\n",
        "\n",
        "  conv3 = Conv2D(filters=64, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)(pool2)\n",
        "  drop3 = Dropout(rate=0.2)(conv3)\n",
        "  pool3 = MaxPool2D(padding='SAME')(drop3)\n",
        "\n",
        "  pool3_flat = keras.layers.Flatten()(pool3)\n",
        "  dense4 = Dense(units=128, activation=tf.nn.relu)(pool3_flat)\n",
        "  logits = Dense(units=3, activation=tf.nn.sigmoid)(dense4)\n",
        "\n",
        "  return keras.Model(inputs=inputs, outputs=logits)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKgrCzcDXBGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = AcousticSoundModel()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIrEr_y7VAKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tALWLYRoO112",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = './model' #경로 설정은 이따가 제대로\n",
        "    \n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2-nGwfG8-a0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = model_dir + '/soundee_classification.model'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)# Create a callback that saves the model's weights\n",
        "cp = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                        monitor='val_loss', mode='min', save_best_only=True, \n",
        "                                        verbose=1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZkHViDNtj-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUH2o7GzVAUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callback = [es, cp]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YSfzlb6VAZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "007377b7-eb38-4023-c2ae-4d09fba9e172"
      },
      "source": [
        "hist = model.fit(\n",
        "      train_ds,\n",
        "      steps_per_epoch=STEPS_PER_EPOCH,\n",
        "      epochs=training_epochs,\n",
        "      batch_size = BATCH_SIZE,\n",
        "      validation_data=val_ds,\n",
        "      validation_steps=VALIDATION_STEPS,\n",
        "      callbacks=callback)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.2276 - acc: 0.9105\n",
            "Epoch 00001: val_loss improved from inf to 0.01928, saving model to ./model/soundee_classification.model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 32s 488ms/step - loss: 0.2276 - acc: 0.9105 - val_loss: 0.0193 - val_acc: 0.9955\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.0137 - acc: 0.9960\n",
            "Epoch 00002: val_loss improved from 0.01928 to 0.01002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 394ms/step - loss: 0.0137 - acc: 0.9960 - val_loss: 0.0100 - val_acc: 1.0000\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9998\n",
            "Epoch 00003: val_loss improved from 0.01002 to 0.00439, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 0.0022 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 1.0000\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9991\n",
            "Epoch 00004: val_loss improved from 0.00439 to 0.00168, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.0017 - val_acc: 1.0000\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.0440 - acc: 0.9910\n",
            "Epoch 00005: val_loss did not improve from 0.00168\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 0.0440 - acc: 0.9910 - val_loss: 0.0165 - val_acc: 0.9933\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.0342 - acc: 0.9927\n",
            "Epoch 00006: val_loss did not improve from 0.00168\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 0.0342 - acc: 0.9927 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9979\n",
            "Epoch 00007: val_loss did not improve from 0.00168\n",
            "66/66 [==============================] - 25s 373ms/step - loss: 0.0076 - acc: 0.9979 - val_loss: 0.0050 - val_acc: 1.0000\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.0031 - acc: 0.9981\n",
            "Epoch 00008: val_loss improved from 0.00168 to 0.00120, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 0.0031 - acc: 0.9981 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.0031 - acc: 0.9974\n",
            "Epoch 00009: val_loss improved from 0.00120 to 0.00032, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 395ms/step - loss: 0.0031 - acc: 0.9974 - val_loss: 3.2253e-04 - val_acc: 1.0000\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 9.8632e-04 - acc: 0.9993\n",
            "Epoch 00010: val_loss improved from 0.00032 to 0.00019, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 390ms/step - loss: 9.8632e-04 - acc: 0.9993 - val_loss: 1.9174e-04 - val_acc: 1.0000\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.6448e-06 - acc: 1.0000\n",
            "Epoch 00011: val_loss improved from 0.00019 to 0.00014, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 390ms/step - loss: 1.6448e-06 - acc: 1.0000 - val_loss: 1.3553e-04 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.7550e-06 - acc: 1.0000\n",
            "Epoch 00012: val_loss improved from 0.00014 to 0.00013, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 1.7550e-06 - acc: 1.0000 - val_loss: 1.3251e-04 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.3686e-06 - acc: 1.0000\n",
            "Epoch 00013: val_loss improved from 0.00013 to 0.00012, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.3686e-06 - acc: 1.0000 - val_loss: 1.2171e-04 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.6656e-06 - acc: 1.0000\n",
            "Epoch 00014: val_loss improved from 0.00012 to 0.00012, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.6656e-06 - acc: 1.0000 - val_loss: 1.1798e-04 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.4183e-06 - acc: 1.0000\n",
            "Epoch 00015: val_loss improved from 0.00012 to 0.00011, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.4183e-06 - acc: 1.0000 - val_loss: 1.0954e-04 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.1890e-06 - acc: 1.0000\n",
            "Epoch 00016: val_loss improved from 0.00011 to 0.00011, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.1890e-06 - acc: 1.0000 - val_loss: 1.0791e-04 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.2884e-06 - acc: 1.0000\n",
            "Epoch 00017: val_loss improved from 0.00011 to 0.00010, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 1.2884e-06 - acc: 1.0000 - val_loss: 1.0309e-04 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.1380e-06 - acc: 1.0000\n",
            "Epoch 00018: val_loss did not improve from 0.00010\n",
            "66/66 [==============================] - 24s 371ms/step - loss: 1.1380e-06 - acc: 1.0000 - val_loss: 1.0698e-04 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.2705e-06 - acc: 1.0000\n",
            "Epoch 00019: val_loss improved from 0.00010 to 0.00009, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.2705e-06 - acc: 1.0000 - val_loss: 9.2567e-05 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 9.8649e-07 - acc: 1.0000\n",
            "Epoch 00020: val_loss improved from 0.00009 to 0.00009, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 9.8649e-07 - acc: 1.0000 - val_loss: 9.0264e-05 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.0002e-06 - acc: 1.0000\n",
            "Epoch 00021: val_loss improved from 0.00009 to 0.00009, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.0002e-06 - acc: 1.0000 - val_loss: 8.6483e-05 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 9.7327e-07 - acc: 1.0000\n",
            "Epoch 00022: val_loss improved from 0.00009 to 0.00008, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 9.7327e-07 - acc: 1.0000 - val_loss: 8.0774e-05 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 9.8298e-07 - acc: 1.0000\n",
            "Epoch 00023: val_loss improved from 0.00008 to 0.00008, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 394ms/step - loss: 9.8298e-07 - acc: 1.0000 - val_loss: 7.8464e-05 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 8.4540e-07 - acc: 1.0000\n",
            "Epoch 00024: val_loss improved from 0.00008 to 0.00008, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 8.4540e-07 - acc: 1.0000 - val_loss: 7.5269e-05 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 7.0634e-07 - acc: 1.0000\n",
            "Epoch 00025: val_loss did not improve from 0.00008\n",
            "66/66 [==============================] - 25s 373ms/step - loss: 7.0634e-07 - acc: 1.0000 - val_loss: 7.9283e-05 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 8.6650e-07 - acc: 1.0000\n",
            "Epoch 00026: val_loss improved from 0.00008 to 0.00007, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 390ms/step - loss: 8.6650e-07 - acc: 1.0000 - val_loss: 7.1075e-05 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 6.6349e-07 - acc: 1.0000\n",
            "Epoch 00027: val_loss improved from 0.00007 to 0.00007, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 6.6349e-07 - acc: 1.0000 - val_loss: 7.0631e-05 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 7.3881e-07 - acc: 1.0000\n",
            "Epoch 00028: val_loss did not improve from 0.00007\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 7.3881e-07 - acc: 1.0000 - val_loss: 7.0732e-05 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 7.0067e-07 - acc: 1.0000\n",
            "Epoch 00029: val_loss improved from 0.00007 to 0.00007, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 7.0067e-07 - acc: 1.0000 - val_loss: 6.5812e-05 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 6.5485e-07 - acc: 1.0000\n",
            "Epoch 00030: val_loss improved from 0.00007 to 0.00007, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 6.5485e-07 - acc: 1.0000 - val_loss: 6.5444e-05 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 7.1097e-07 - acc: 1.0000\n",
            "Epoch 00031: val_loss improved from 0.00007 to 0.00006, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 395ms/step - loss: 7.1097e-07 - acc: 1.0000 - val_loss: 6.1141e-05 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 6.0371e-07 - acc: 1.0000\n",
            "Epoch 00032: val_loss improved from 0.00006 to 0.00006, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 6.0371e-07 - acc: 1.0000 - val_loss: 5.8995e-05 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 5.9012e-07 - acc: 1.0000\n",
            "Epoch 00033: val_loss improved from 0.00006 to 0.00006, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 390ms/step - loss: 5.9012e-07 - acc: 1.0000 - val_loss: 5.7582e-05 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 6.0911e-07 - acc: 1.0000\n",
            "Epoch 00034: val_loss improved from 0.00006 to 0.00006, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 6.0911e-07 - acc: 1.0000 - val_loss: 5.6324e-05 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 5.5972e-07 - acc: 1.0000\n",
            "Epoch 00035: val_loss did not improve from 0.00006\n",
            "66/66 [==============================] - 25s 374ms/step - loss: 5.5972e-07 - acc: 1.0000 - val_loss: 6.2034e-05 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 5.5598e-07 - acc: 1.0000\n",
            "Epoch 00036: val_loss did not improve from 0.00006\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 5.5598e-07 - acc: 1.0000 - val_loss: 8.4348e-05 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 5.1552e-07 - acc: 1.0000\n",
            "Epoch 00037: val_loss improved from 0.00006 to 0.00005, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 394ms/step - loss: 5.1552e-07 - acc: 1.0000 - val_loss: 5.2129e-05 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.7009e-07 - acc: 1.0000\n",
            "Epoch 00038: val_loss did not improve from 0.00005\n",
            "66/66 [==============================] - 25s 374ms/step - loss: 4.7009e-07 - acc: 1.0000 - val_loss: 5.2874e-05 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 5.0226e-07 - acc: 1.0000\n",
            "Epoch 00039: val_loss improved from 0.00005 to 0.00005, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 5.0226e-07 - acc: 1.0000 - val_loss: 4.9444e-05 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.9557e-07 - acc: 1.0000\n",
            "Epoch 00040: val_loss did not improve from 0.00005\n",
            "66/66 [==============================] - 25s 373ms/step - loss: 4.9557e-07 - acc: 1.0000 - val_loss: 5.1655e-05 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.8503e-07 - acc: 1.0000\n",
            "Epoch 00041: val_loss improved from 0.00005 to 0.00005, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 4.8503e-07 - acc: 1.0000 - val_loss: 4.6466e-05 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.3404e-07 - acc: 1.0000\n",
            "Epoch 00042: val_loss did not improve from 0.00005\n",
            "66/66 [==============================] - 25s 371ms/step - loss: 4.3404e-07 - acc: 1.0000 - val_loss: 4.8248e-05 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.5283e-07 - acc: 1.0000\n",
            "Epoch 00043: val_loss improved from 0.00005 to 0.00005, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 4.5283e-07 - acc: 1.0000 - val_loss: 4.5563e-05 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.2679e-07 - acc: 1.0000\n",
            "Epoch 00044: val_loss improved from 0.00005 to 0.00004, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 4.2679e-07 - acc: 1.0000 - val_loss: 4.3475e-05 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.3039e-07 - acc: 1.0000\n",
            "Epoch 00045: val_loss did not improve from 0.00004\n",
            "66/66 [==============================] - 25s 373ms/step - loss: 4.3039e-07 - acc: 1.0000 - val_loss: 4.6449e-05 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.9340e-07 - acc: 1.0000\n",
            "Epoch 00046: val_loss did not improve from 0.00004\n",
            "66/66 [==============================] - 25s 373ms/step - loss: 3.9340e-07 - acc: 1.0000 - val_loss: 6.2783e-05 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.5769e-07 - acc: 1.0000\n",
            "Epoch 00047: val_loss improved from 0.00004 to 0.00004, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 3.5769e-07 - acc: 1.0000 - val_loss: 4.0672e-05 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.6879e-07 - acc: 1.0000\n",
            "Epoch 00048: val_loss improved from 0.00004 to 0.00004, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 3.6879e-07 - acc: 1.0000 - val_loss: 4.0558e-05 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.7286e-07 - acc: 1.0000\n",
            "Epoch 00049: val_loss improved from 0.00004 to 0.00004, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 3.7286e-07 - acc: 1.0000 - val_loss: 3.9505e-05 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.5636e-07 - acc: 1.0000\n",
            "Epoch 00050: val_loss improved from 0.00004 to 0.00004, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 3.5636e-07 - acc: 1.0000 - val_loss: 3.8038e-05 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.4294e-07 - acc: 1.0000\n",
            "Epoch 00051: val_loss improved from 0.00004 to 0.00004, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 3.4294e-07 - acc: 1.0000 - val_loss: 3.7093e-05 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.4423e-07 - acc: 1.0000\n",
            "Epoch 00052: val_loss did not improve from 0.00004\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 3.4423e-07 - acc: 1.0000 - val_loss: 4.0311e-05 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.3367e-07 - acc: 1.0000\n",
            "Epoch 00053: val_loss did not improve from 0.00004\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 3.3367e-07 - acc: 1.0000 - val_loss: 3.7529e-05 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.2189e-07 - acc: 1.0000\n",
            "Epoch 00054: val_loss improved from 0.00004 to 0.00003, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 3.2189e-07 - acc: 1.0000 - val_loss: 3.4822e-05 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.0375e-07 - acc: 1.0000\n",
            "Epoch 00055: val_loss improved from 0.00003 to 0.00003, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 3.0375e-07 - acc: 1.0000 - val_loss: 3.3818e-05 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.9021e-07 - acc: 1.0000\n",
            "Epoch 00056: val_loss improved from 0.00003 to 0.00003, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 2.9021e-07 - acc: 1.0000 - val_loss: 3.3284e-05 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.0811e-07 - acc: 1.0000\n",
            "Epoch 00057: val_loss improved from 0.00003 to 0.00003, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 395ms/step - loss: 3.0811e-07 - acc: 1.0000 - val_loss: 3.2928e-05 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.8733e-07 - acc: 1.0000\n",
            "Epoch 00058: val_loss did not improve from 0.00003\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 2.8733e-07 - acc: 1.0000 - val_loss: 4.8335e-05 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.9560e-07 - acc: 1.0000\n",
            "Epoch 00059: val_loss improved from 0.00003 to 0.00003, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 2.9560e-07 - acc: 1.0000 - val_loss: 3.1198e-05 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.8603e-07 - acc: 1.0000\n",
            "Epoch 00060: val_loss improved from 0.00003 to 0.00003, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 2.8603e-07 - acc: 1.0000 - val_loss: 3.0782e-05 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.5796e-07 - acc: 1.0000\n",
            "Epoch 00061: val_loss improved from 0.00003 to 0.00003, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 394ms/step - loss: 2.5796e-07 - acc: 1.0000 - val_loss: 3.0192e-05 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.5538e-07 - acc: 1.0000\n",
            "Epoch 00062: val_loss improved from 0.00003 to 0.00003, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 2.5538e-07 - acc: 1.0000 - val_loss: 2.9524e-05 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.7545e-07 - acc: 1.0000\n",
            "Epoch 00063: val_loss improved from 0.00003 to 0.00003, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 2.7545e-07 - acc: 1.0000 - val_loss: 2.8898e-05 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.5046e-07 - acc: 1.0000\n",
            "Epoch 00064: val_loss did not improve from 0.00003\n",
            "66/66 [==============================] - 24s 371ms/step - loss: 2.5046e-07 - acc: 1.0000 - val_loss: 2.9146e-05 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.4186e-07 - acc: 1.0000\n",
            "Epoch 00065: val_loss did not improve from 0.00003\n",
            "66/66 [==============================] - 25s 374ms/step - loss: 2.4186e-07 - acc: 1.0000 - val_loss: 4.1352e-05 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.5915e-07 - acc: 1.0000\n",
            "Epoch 00066: val_loss did not improve from 0.00003\n",
            "66/66 [==============================] - 24s 371ms/step - loss: 2.5915e-07 - acc: 1.0000 - val_loss: 2.9588e-05 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.4023e-07 - acc: 1.0000\n",
            "Epoch 00067: val_loss improved from 0.00003 to 0.00003, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 2.4023e-07 - acc: 1.0000 - val_loss: 2.6450e-05 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.3187e-07 - acc: 1.0000\n",
            "Epoch 00068: val_loss did not improve from 0.00003\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 2.3187e-07 - acc: 1.0000 - val_loss: 2.8537e-05 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.2912e-07 - acc: 1.0000\n",
            "Epoch 00069: val_loss did not improve from 0.00003\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 2.2912e-07 - acc: 1.0000 - val_loss: 2.6913e-05 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.2291e-07 - acc: 1.0000\n",
            "Epoch 00070: val_loss improved from 0.00003 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 2.2291e-07 - acc: 1.0000 - val_loss: 2.4737e-05 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.2374e-07 - acc: 1.0000\n",
            "Epoch 00071: val_loss did not improve from 0.00002\n",
            "66/66 [==============================] - 24s 371ms/step - loss: 2.2374e-07 - acc: 1.0000 - val_loss: 2.5225e-05 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.2885e-07 - acc: 1.0000\n",
            "Epoch 00072: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 2.2885e-07 - acc: 1.0000 - val_loss: 2.3816e-05 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.1873e-07 - acc: 1.0000\n",
            "Epoch 00073: val_loss did not improve from 0.00002\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 2.1873e-07 - acc: 1.0000 - val_loss: 3.4313e-05 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.1494e-07 - acc: 1.0000\n",
            "Epoch 00074: val_loss did not improve from 0.00002\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 2.1494e-07 - acc: 1.0000 - val_loss: 3.3645e-05 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.1226e-07 - acc: 1.0000\n",
            "Epoch 00075: val_loss did not improve from 0.00002\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 2.1226e-07 - acc: 1.0000 - val_loss: 2.4225e-05 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.0536e-07 - acc: 1.0000\n",
            "Epoch 00076: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 2.0536e-07 - acc: 1.0000 - val_loss: 2.3367e-05 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.9920e-07 - acc: 1.0000\n",
            "Epoch 00077: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 394ms/step - loss: 1.9920e-07 - acc: 1.0000 - val_loss: 2.1402e-05 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.0667e-07 - acc: 1.0000\n",
            "Epoch 00078: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 2.0667e-07 - acc: 1.0000 - val_loss: 2.0984e-05 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.9261e-07 - acc: 1.0000\n",
            "Epoch 00079: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 1.9261e-07 - acc: 1.0000 - val_loss: 2.0450e-05 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.9209e-07 - acc: 1.0000\n",
            "Epoch 00080: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 1.9209e-07 - acc: 1.0000 - val_loss: 2.0024e-05 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.8928e-07 - acc: 1.0000\n",
            "Epoch 00081: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.8928e-07 - acc: 1.0000 - val_loss: 1.9690e-05 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.8320e-07 - acc: 1.0000\n",
            "Epoch 00082: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 390ms/step - loss: 1.8320e-07 - acc: 1.0000 - val_loss: 1.9451e-05 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.8999e-07 - acc: 1.0000\n",
            "Epoch 00083: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 394ms/step - loss: 1.8999e-07 - acc: 1.0000 - val_loss: 1.9024e-05 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.7553e-07 - acc: 1.0000\n",
            "Epoch 00084: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 1.7553e-07 - acc: 1.0000 - val_loss: 1.8600e-05 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.8934e-07 - acc: 1.0000\n",
            "Epoch 00085: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 1.8934e-07 - acc: 1.0000 - val_loss: 1.8301e-05 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.6998e-07 - acc: 1.0000\n",
            "Epoch 00086: val_loss did not improve from 0.00002\n",
            "66/66 [==============================] - 25s 373ms/step - loss: 1.6998e-07 - acc: 1.0000 - val_loss: 1.9928e-05 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.7207e-07 - acc: 1.0000\n",
            "Epoch 00087: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 394ms/step - loss: 1.7207e-07 - acc: 1.0000 - val_loss: 1.7420e-05 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.7647e-07 - acc: 1.0000\n",
            "Epoch 00088: val_loss did not improve from 0.00002\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 1.7647e-07 - acc: 1.0000 - val_loss: 1.8761e-05 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.7191e-07 - acc: 1.0000\n",
            "Epoch 00089: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.7191e-07 - acc: 1.0000 - val_loss: 1.7095e-05 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.7324e-07 - acc: 1.0000\n",
            "Epoch 00090: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.7324e-07 - acc: 1.0000 - val_loss: 1.6724e-05 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.6298e-07 - acc: 1.0000\n",
            "Epoch 00091: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 391ms/step - loss: 1.6298e-07 - acc: 1.0000 - val_loss: 1.6420e-05 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.7495e-07 - acc: 1.0000\n",
            "Epoch 00092: val_loss did not improve from 0.00002\n",
            "66/66 [==============================] - 25s 372ms/step - loss: 1.7495e-07 - acc: 1.0000 - val_loss: 1.7496e-05 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.5875e-07 - acc: 1.0000\n",
            "Epoch 00093: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 394ms/step - loss: 1.5875e-07 - acc: 1.0000 - val_loss: 1.6149e-05 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.6504e-07 - acc: 1.0000\n",
            "Epoch 00094: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.6504e-07 - acc: 1.0000 - val_loss: 1.5907e-05 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.6040e-07 - acc: 1.0000\n",
            "Epoch 00095: val_loss improved from 0.00002 to 0.00002, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.6040e-07 - acc: 1.0000 - val_loss: 1.5773e-05 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.5302e-07 - acc: 1.0000\n",
            "Epoch 00096: val_loss improved from 0.00002 to 0.00001, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 393ms/step - loss: 1.5302e-07 - acc: 1.0000 - val_loss: 1.4947e-05 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.5896e-07 - acc: 1.0000\n",
            "Epoch 00097: val_loss did not improve from 0.00001\n",
            "66/66 [==============================] - 25s 373ms/step - loss: 1.5896e-07 - acc: 1.0000 - val_loss: 1.5872e-05 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.5525e-07 - acc: 1.0000\n",
            "Epoch 00098: val_loss did not improve from 0.00001\n",
            "66/66 [==============================] - 25s 373ms/step - loss: 1.5525e-07 - acc: 1.0000 - val_loss: 1.5944e-05 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.5535e-07 - acc: 1.0000\n",
            "Epoch 00099: val_loss improved from 0.00001 to 0.00001, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.5535e-07 - acc: 1.0000 - val_loss: 1.4226e-05 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.5446e-07 - acc: 1.0000\n",
            "Epoch 00100: val_loss improved from 0.00001 to 0.00001, saving model to ./model/soundee_classification.model\n",
            "INFO:tensorflow:Assets written to: ./model/soundee_classification.model/assets\n",
            "66/66 [==============================] - 26s 392ms/step - loss: 1.5446e-07 - acc: 1.0000 - val_loss: 1.3866e-05 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no5xv9Ew97Rb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a9887866-c243-4d69-f1fd-1949f0083349"
      },
      "source": [
        "!ls {checkpoint_dir} # 저장된 checkpoint 확인하기"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "soundee_classification.model.data-00000-of-00002\n",
            "soundee_classification.model.data-00001-of-00002\n",
            "soundee_classification.model.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBTTi_4GZJgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.save_weights('saveweights', save_format='tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZdvfDCsZvFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "bb7d642b-3e28-4e4c-aeb4-6a93b2e62d1d"
      },
      "source": [
        "acc = hist.history['acc']\n",
        "val_acc = hist.history['val_acc']\n",
        "loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfXrBnLk_eO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "35c01994-2585-4048-f484-9d3046f72627"
      },
      "source": [
        "loss, acc = model.evaluate(test_ds, batch_size=BATCH_SIZE, steps=TEST_STEPS)\n",
        "print('acc: %.2f%%, loss: %f'% (acc*100, loss))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 1s 100ms/step - loss: 2.9685e-04 - acc: 1.0000\n",
            "acc: 100.00%, loss: 0.000297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqH-L7qm8U_R",
        "colab_type": "text"
      },
      "source": [
        "prediction test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aOuWAZ-8TTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "#import os, glob, numpy as np\n",
        "from keras.models import load_model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsNgzwxBvdi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d6864e29-1aef-4b78-fcb6-a5a02ef41c3a"
      },
      "source": [
        "test_dir = tf.keras.utils.get_file(origin='https://docs.google.com/uc?export=download&id=1VMllFJGjUwTScLMSpYJAR7gqjtQGu7fH', \n",
        "                                   fname='test', extract=True)\n",
        "test_dir = pathlib.Path(test_dir)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://docs.google.com/uc?export=download&id=1VMllFJGjUwTScLMSpYJAR7gqjtQGu7fH\n",
            "983040/978684 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TrsypJ8vuFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test_dir = pathlib.PosixPath(\"/root/.keras/datasets/DATASET\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXPBEY6uNIrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = \"/root/.keras/datasets/DATASET\""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd-XzmmjO6d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "filenames = []\n",
        "files = glob(test_dir+\"/*.png\")\n",
        "for i, f in enumerate(files):\n",
        "    img = Image.open(f)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "    data = np.asarray(img)\n",
        "    filenames.append(f)\n",
        "    X.append(data)\n",
        "\n",
        "X = np.array(X, dtype=np.float32)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n-G-pvOxy28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "897066e5-dbff-4966-ddb5-20bf81692248"
      },
      "source": [
        "files[:16]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/root/.keras/datasets/DATASET/MOTOR_2.png',\n",
              " '/root/.keras/datasets/DATASET/MOTOR_5.png',\n",
              " '/root/.keras/datasets/DATASET/WATER_3.png',\n",
              " '/root/.keras/datasets/DATASET/MOTOR_0.png',\n",
              " '/root/.keras/datasets/DATASET/DROP_5.png',\n",
              " '/root/.keras/datasets/DATASET/DROP_1.png',\n",
              " '/root/.keras/datasets/DATASET/MOTOR_1.png',\n",
              " '/root/.keras/datasets/DATASET/DROP_2.png',\n",
              " '/root/.keras/datasets/DATASET/WATER_0.png',\n",
              " '/root/.keras/datasets/DATASET/WATER_4.png',\n",
              " '/root/.keras/datasets/DATASET/DROP_4.png',\n",
              " '/root/.keras/datasets/DATASET/WATER_2.png',\n",
              " '/root/.keras/datasets/DATASET/DROP_0.png',\n",
              " '/root/.keras/datasets/DATASET/WATER_5.png',\n",
              " '/root/.keras/datasets/DATASET/WATER_1.png',\n",
              " '/root/.keras/datasets/DATASET/MOTOR_3.png']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GLVcO-vQVOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.predict(X)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9KR3__95D1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9d1fd9f9-dd1c-4741-cd16-bf094d105a79"
      },
      "source": [
        "filenames[12].split('/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', 'root', '.keras', 'datasets', 'DATASET', 'DROP_3.png']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czcJ7wWCQ_ZY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        },
        "outputId": "bd2baf80-df61-4396-c3f2-7e390ef5f47c"
      },
      "source": [
        "cnt = 0\n",
        "for i in prediction:\n",
        "    pre_ans = i.argmax()  # 예측 레이블\n",
        "    print(i)\n",
        "    pre_ans_str = ''\n",
        "    if pre_ans == 0: pre_ans_str = \"motor\"\n",
        "    elif pre_ans == 1: pre_ans_str = \"water\"\n",
        "    elif pre_ans == 2: pre_ans_str = \"drop\"\n",
        "    #elif pre_ans == 3: pre_ans_str = \"baby\"\n",
        "    #else: pre_ans_str = \"dog\"\n",
        "\n",
        "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split('/')[5]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split('/')[5]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split('/')[5]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    #if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split('/')[5]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    #if i[4] >= 0.8: print(\"해당 \"+filenames[cnt].split('/')[5]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[0] < 0.8 and i[1] < 0.8 and i[2] < 0.8 : print(\"추정할 수 없습니다.\")\n",
        "    print()\n",
        "    cnt += 1"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.000 1.000 0.000]\n",
            "해당 MOTOR_2.png이미지는 water로 추정됩니다.\n",
            "\n",
            "[0.000 0.000 1.000]\n",
            "해당 MOTOR_5.png이미지는 drop로 추정됩니다.\n",
            "\n",
            "[1.000 0.000 0.000]\n",
            "해당 WATER_3.png이미지는 motor로 추정됩니다.\n",
            "\n",
            "[0.000 1.000 1.000]\n",
            "해당 MOTOR_0.png이미지는 water로 추정됩니다.\n",
            "해당 MOTOR_0.png이미지는 water로 추정됩니다.\n",
            "\n",
            "[0.000 1.000 0.000]\n",
            "해당 DROP_5.png이미지는 water로 추정됩니다.\n",
            "\n",
            "[0.000 1.000 0.000]\n",
            "해당 DROP_1.png이미지는 water로 추정됩니다.\n",
            "\n",
            "[0.000 1.000 0.000]\n",
            "해당 MOTOR_1.png이미지는 water로 추정됩니다.\n",
            "\n",
            "[0.000 1.000 0.000]\n",
            "해당 DROP_2.png이미지는 water로 추정됩니다.\n",
            "\n",
            "[1.000 0.000 0.000]\n",
            "해당 WATER_0.png이미지는 motor로 추정됩니다.\n",
            "\n",
            "[1.000 0.000 0.000]\n",
            "해당 WATER_4.png이미지는 motor로 추정됩니다.\n",
            "\n",
            "[0.000 1.000 0.000]\n",
            "해당 DROP_4.png이미지는 water로 추정됩니다.\n",
            "\n",
            "[0.000 1.000 0.000]\n",
            "해당 WATER_2.png이미지는 water로 추정됩니다.\n",
            "\n",
            "[0.000 1.000 0.000]\n",
            "해당 DROP_0.png이미지는 water로 추정됩니다.\n",
            "\n",
            "[1.000 0.000 0.000]\n",
            "해당 WATER_5.png이미지는 motor로 추정됩니다.\n",
            "\n",
            "[0.000 0.000 0.000]\n",
            "추정할 수 없습니다.\n",
            "\n",
            "[0.000 0.000 1.000]\n",
            "해당 MOTOR_3.png이미지는 drop로 추정됩니다.\n",
            "\n",
            "[0.000 1.000 0.000]\n",
            "해당 DROP_3.png이미지는 water로 추정됩니다.\n",
            "\n",
            "[0.000 0.000 1.000]\n",
            "해당 MOTOR_4.png이미지는 drop로 추정됩니다.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWuRiJ4-8Tiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRJHAx_l8Tet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U3QGlUY8Tc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gUb99RH8TZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx2uRe1X8TXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QZWw5nu_Gt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = model.predict(test_ds, steps=TEST_STEPS)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8c3usQn_Gwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in output:\n",
        "    pre_ans = i.argmax()  # 예측 레이블\n",
        "    print(i)\n",
        "    #print(pre_ans)\n",
        "    pre_ans_str = ''\n",
        "    if pre_ans == 0: pre_ans_str = \"motor\"\n",
        "    elif pre_ans == 1: pre_ans_str = \"water\"\n",
        "    elif pre_ans == 2: pre_ans_str = \"drop\"\n",
        "    elif pre_ans == 3: pre_ans_str = \"baby\"\n",
        "    else: pre_ans_str = \"dog\"\n",
        "\n",
        "    if i[0] >= 0.8 : print(\"해당 이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    #else : print(\"추정하지 못했습니다.\")\n",
        "    if i[1] >= 0.8: print(\"해당 이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    #else : print(\"추정하지 못했습니다.\")\n",
        "    if i[2] >= 0.8: print(\"해당 이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    #else : print(\"추정하지 못했습니다.\")\n",
        "    if i[3] >= 0.8: print(\"해당 이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    #else : print(\"추정하지 못했습니다.\")\n",
        "    if i[4] >= 0.8: print(\"해당 이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    #else : print(\"추정하지 못했습니다.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9QsITJrNbNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gCb0bESNbQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "#import os, glob, numpy as np\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M4wnOWcNiDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = \"./multi_img_data/imgs_others_test\" #test 이미지 넣어둔 디렉토리 경로. git에서 받아와서 설정하면됨"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf62XEqQNiGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('soundee_classification.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6EibquwFtqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_model = AcousticSoundModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiJwKgKD_qgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0528b63d-eb7b-4006-e631-77df37f11558"
      },
      "source": [
        "test_model.load_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa8500d7668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_1xUX93bnrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdeo7OMCRMQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "3c4e4031-9c0c-4273-927c-42680b065b88"
      },
      "source": [
        "filenames[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/root/.keras/datasets/DATASET/DROP_0.png',\n",
              " '/root/.keras/datasets/DATASET/MOTOR_2.png',\n",
              " '/root/.keras/datasets/DATASET/WATER_0.png',\n",
              " '/root/.keras/datasets/DATASET/WATER_1.png',\n",
              " '/root/.keras/datasets/DATASET/MOTOR_0.png',\n",
              " '/root/.keras/datasets/DATASET/WATER_2.png',\n",
              " '/root/.keras/datasets/DATASET/MOTOR_1.png',\n",
              " '/root/.keras/datasets/DATASET/DROP_2.png',\n",
              " '/root/.keras/datasets/DATASET/DROP_1.png']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL8FONP0Q_ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2MNw7mJQ_pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS4IGiU1Q_ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxW1hQncNs1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = test_dir[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXbNOdQONShw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_pil = Image.open(path)\n",
        "image = np.array(image_pil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KZv0FQBN40a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d76e45a2-7a51-452a-ba2b-c897c653cfe8"
      },
      "source": [
        "image.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(770, 310, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt5GZJZ9N-up",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6e6882d4-836f-4dbd-aa56-6a2097dcab57"
      },
      "source": [
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJD9RF1ZzvUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = tf.data.Dataset.list_files(str(test_dir/'*'))\n",
        "test = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "test = prepare_for_training(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDe9MGDRz-w2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "d51bc1ef-66ba-4662-c6ca-13f8a198a136"
      },
      "source": [
        "prediction = model.predict(test, steps=TEST_STEPS)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.000 0.000 0.000]\n",
            " [0.000 0.000 0.249]\n",
            " [1.000 0.000 0.000]\n",
            " ...\n",
            " [0.000 0.000 0.894]\n",
            " [0.000 0.000 0.008]\n",
            " [1.000 0.000 0.000]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXA2Bp071Z5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "ade67e84-7564-45df-bb88-d0532f4e7cdd"
      },
      "source": [
        "filenames = tf.data.Dataset.list_files(str(test_dir/'*'))\n",
        "for f in filenames.take(8):\n",
        "  print(f.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'/root/.keras/datasets/DATASET/WATER_0.png'\n",
            "b'/root/.keras/datasets/DATASET/MOTOR_1.png'\n",
            "b'/root/.keras/datasets/DATASET/MOTOR_2.png'\n",
            "b'/root/.keras/datasets/DATASET/DROP_0.png'\n",
            "b'/root/.keras/datasets/DATASET/MOTOR_0.png'\n",
            "b'/root/.keras/datasets/DATASET/WATER_1.png'\n",
            "b'/root/.keras/datasets/DATASET/DROP_1.png'\n",
            "b'/root/.keras/datasets/DATASET/DROP_2.png'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHLk_cz52bzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnt = 0\n",
        "for i in prediction:\n",
        "    pre_ans = i.argmax()  # 예측 레이블\n",
        "    print(i)\n",
        "    print(pre_ans)\n",
        "    pre_ans_str = ''\n",
        "    if pre_ans == 0: pre_ans_str = \"motor\"\n",
        "    elif pre_ans == 1: pre_ans_str = \"water\"\n",
        "    else: pre_ans_str = \"drop\"\n",
        "    if i[0] >= 0.8 : print(\"해당 이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[1] >= 0.8: print(\"해당 이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[2] >= 0.8: print(\"해당 이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    cnt += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cGW13ll0fPt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "e83808bf-d6d5-4475-e2e4-4eb1ed65d5bf"
      },
      "source": [
        "cnt = 0\n",
        "for i in prediction:\n",
        "    pre_ans = i.argmax()  # 예측 레이블\n",
        "    print(i)\n",
        "    print(pre_ans)\n",
        "    pre_ans_str = ''\n",
        "    if pre_ans == 0: pre_ans_str = \"motor\"\n",
        "    elif pre_ans == 1: pre_ans_str = \"water\"\n",
        "    else: pre_ans_str = \"drop\"\n",
        "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
        "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    cnt += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.000 0.000 0.000]\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-b0f65d35215d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpre_ans\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpre_ans_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"water\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpre_ans_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"해당 \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"이미지는 \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpre_ans_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"로 추정됩니다.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"해당 \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"이미지는 \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpre_ans_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"으로 추정됩니다.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"해당 \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"이미지는 \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpre_ans_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"로 추정됩니다.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FpKytyAxhbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_w = 64\n",
        "image_h = 64\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "X = []\n",
        "filenames = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5lN-WJHxhe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = test_dir.glob('/*.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viGbDNgaNiK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "d51697ba-0607-4922-93fb-d591504a297f"
      },
      "source": [
        "\n",
        "#model = load_model('./model/soundee_classification.model')\n",
        "\n",
        "prediction = model.predict(X)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "cnt = 0\n",
        "\n",
        "#이 비교는 그냥 파일들이 있으면 해당 파일과 비교. 카테고리와 함께 비교해서 진행하는 것은 _4 파일.\n",
        "for i in prediction:\n",
        "    pre_ans = i.argmax()  # 예측 레이블\n",
        "    print(i)\n",
        "    print(pre_ans)\n",
        "    pre_ans_str = ''\n",
        "    if pre_ans == 0: pre_ans_str = \"motor\"\n",
        "    elif pre_ans == 1: pre_ans_str = \"drop\"\n",
        "    elif pre_ans == 2: pre_ans_str = \"water\"\n",
        "    elif pre_ans == 3: pre_ans_str = \"babycrying\"\n",
        "    else: pre_ans_str = \"dog\"\n",
        "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
        "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[4] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    cnt += 1\n",
        "    # print(i.argmax()) #얘가 레이블 [1. 0. 0.] 이런식으로 되어 있는 것을 숫자로 바꿔주는 것.\n",
        "    # 즉 얘랑, 나중에 카테고리 데이터 불러와서 카테고리랑 비교를 해서 같으면 맞는거고, 아니면 틀린거로 취급하면 된다.\n",
        "    # 이걸 한 것은 _4.py에."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-5fd1b8966b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#model = load_model('./model/soundee_classification.model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"{0:0.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m     \u001b[0mall_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'batch_outputs' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxf7UWpcNiOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}